{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "For the machine learning section of the Capstone, three classifiers will be compared to find which performs best. These classifiers were chosen due to their performance regarding larger datasets with many features(for this dataset: ~5k features, ~7.5k samples). The classifiers are as follows: \n",
    "\n",
    "    - Random Forest\n",
    "    - Support Vector Machine\n",
    "    - Neural Network\n",
    " \n",
    "The goal of the classifiers is to predict which team will win given certain features/indicators within the game. The classifiers will be evaluated based on how many game outcomes they predicted correctly. THese results will be displayed using a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wrangled dataframe\n",
    "league_wrangled = pd.read_csv(\"../Dataset/Wrangled_LeagueofLegends.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bResult</th>\n",
       "      <th>gamelength</th>\n",
       "      <th>golddiff</th>\n",
       "      <th>goldblue</th>\n",
       "      <th>bKills</th>\n",
       "      <th>bInhibs</th>\n",
       "      <th>bBarons</th>\n",
       "      <th>bHeralds</th>\n",
       "      <th>goldred</th>\n",
       "      <th>rKills</th>\n",
       "      <th>...</th>\n",
       "      <th>bbot_base</th>\n",
       "      <th>rtop_outer</th>\n",
       "      <th>rtop_inner</th>\n",
       "      <th>rtop_base</th>\n",
       "      <th>rmiddle_outer</th>\n",
       "      <th>rmiddle_inner</th>\n",
       "      <th>rbmiddle_base</th>\n",
       "      <th>rbot_outer</th>\n",
       "      <th>rbot_inner</th>\n",
       "      <th>rbot_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>4.127967e+06</td>\n",
       "      <td>3.594454e+08</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.932450e+08</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.288</td>\n",
       "      <td>39.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.681</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>9.550643e+05</td>\n",
       "      <td>2.983951e+08</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.739425e+08</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.409</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.257</td>\n",
       "      <td>29.799</td>\n",
       "      <td>29.993</td>\n",
       "      <td>15.206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>1.647345e+07</td>\n",
       "      <td>4.426966e+08</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.926390e+08</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>37.109</td>\n",
       "      <td>30.493</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.620</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>3.315823e+06</td>\n",
       "      <td>2.912887e+08</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.329506e+08</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.760</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.122</td>\n",
       "      <td>27.034</td>\n",
       "      <td>31.665</td>\n",
       "      <td>29.969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>8.653433e+06</td>\n",
       "      <td>2.863538e+08</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.126620e+08</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>33.925</td>\n",
       "      <td>15.252</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.644</td>\n",
       "      <td>31.490</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5720 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bResult  gamelength      golddiff      goldblue  bKills  bInhibs  bBarons  \\\n",
       "0        1          40  4.127967e+06  3.594454e+08      16        2        0   \n",
       "1        0          38  9.550643e+05  2.983951e+08      10        0        1   \n",
       "2        1          40  1.647345e+07  4.426966e+08      22        4        1   \n",
       "3        0          41  3.315823e+06  2.912887e+08      10        0        1   \n",
       "4        1          35  8.653433e+06  2.863538e+08      22        1        1   \n",
       "\n",
       "   bHeralds       goldred  rKills  ...  bbot_base  rtop_outer  rtop_inner  \\\n",
       "0         0  2.932450e+08       9  ...      0.000      15.288       39.23   \n",
       "1         0  2.739425e+08       9  ...      0.000      23.409        0.00   \n",
       "2         0  2.926390e+08       8  ...     37.109      30.493        0.00   \n",
       "3         0  3.329506e+08      21  ...      0.000      20.760        0.00   \n",
       "4         0  2.126620e+08      10  ...     33.925      15.252        0.00   \n",
       "\n",
       "   rtop_base  rmiddle_outer  rmiddle_inner  rbmiddle_base  rbot_outer  \\\n",
       "0        0.0         20.681          0.000          0.000      17.237   \n",
       "1        0.0         19.257         29.799         29.993      15.206   \n",
       "2        0.0         24.620          0.000          0.000       0.000   \n",
       "3        0.0         19.122         27.034         31.665      29.969   \n",
       "4        0.0         11.644         31.490          0.000      12.438   \n",
       "\n",
       "   rbot_inner  rbot_base  \n",
       "0         0.0        0.0  \n",
       "1         0.0        0.0  \n",
       "2         0.0        0.0  \n",
       "3         0.0        0.0  \n",
       "4         0.0        0.0  \n",
       "\n",
       "[5 rows x 5720 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View dataframe\n",
    "league_wrangled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate labels from training data\n",
    "result = league_wrangled.pop('bResult')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gamelength</th>\n",
       "      <th>golddiff</th>\n",
       "      <th>goldblue</th>\n",
       "      <th>bKills</th>\n",
       "      <th>bInhibs</th>\n",
       "      <th>bBarons</th>\n",
       "      <th>bHeralds</th>\n",
       "      <th>goldred</th>\n",
       "      <th>rKills</th>\n",
       "      <th>rInhibs</th>\n",
       "      <th>...</th>\n",
       "      <th>bbot_base</th>\n",
       "      <th>rtop_outer</th>\n",
       "      <th>rtop_inner</th>\n",
       "      <th>rtop_base</th>\n",
       "      <th>rmiddle_outer</th>\n",
       "      <th>rmiddle_inner</th>\n",
       "      <th>rbmiddle_base</th>\n",
       "      <th>rbot_outer</th>\n",
       "      <th>rbot_inner</th>\n",
       "      <th>rbot_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>4.127967e+06</td>\n",
       "      <td>3.594454e+08</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.932450e+08</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>15.288</td>\n",
       "      <td>39.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.681</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>9.550643e+05</td>\n",
       "      <td>2.983951e+08</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.739425e+08</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.409</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.257</td>\n",
       "      <td>29.799</td>\n",
       "      <td>29.993</td>\n",
       "      <td>15.206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>1.647345e+07</td>\n",
       "      <td>4.426966e+08</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.926390e+08</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>37.109</td>\n",
       "      <td>30.493</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.620</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>3.315823e+06</td>\n",
       "      <td>2.912887e+08</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.329506e+08</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.760</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.122</td>\n",
       "      <td>27.034</td>\n",
       "      <td>31.665</td>\n",
       "      <td>29.969</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>8.653433e+06</td>\n",
       "      <td>2.863538e+08</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.126620e+08</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.925</td>\n",
       "      <td>15.252</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.644</td>\n",
       "      <td>31.490</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5719 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gamelength      golddiff      goldblue  bKills  bInhibs  bBarons  bHeralds  \\\n",
       "0          40  4.127967e+06  3.594454e+08      16        2        0         0   \n",
       "1          38  9.550643e+05  2.983951e+08      10        0        1         0   \n",
       "2          40  1.647345e+07  4.426966e+08      22        4        1         0   \n",
       "3          41  3.315823e+06  2.912887e+08      10        0        1         0   \n",
       "4          35  8.653433e+06  2.863538e+08      22        1        1         0   \n",
       "\n",
       "        goldred  rKills  rInhibs  ...  bbot_base  rtop_outer  rtop_inner  \\\n",
       "0  2.932450e+08       9        0  ...      0.000      15.288       39.23   \n",
       "1  2.739425e+08       9        1  ...      0.000      23.409        0.00   \n",
       "2  2.926390e+08       8        0  ...     37.109      30.493        0.00   \n",
       "3  3.329506e+08      21        1  ...      0.000      20.760        0.00   \n",
       "4  2.126620e+08      10        0  ...     33.925      15.252        0.00   \n",
       "\n",
       "   rtop_base  rmiddle_outer  rmiddle_inner  rbmiddle_base  rbot_outer  \\\n",
       "0        0.0         20.681          0.000          0.000      17.237   \n",
       "1        0.0         19.257         29.799         29.993      15.206   \n",
       "2        0.0         24.620          0.000          0.000       0.000   \n",
       "3        0.0         19.122         27.034         31.665      29.969   \n",
       "4        0.0         11.644         31.490          0.000      12.438   \n",
       "\n",
       "   rbot_inner  rbot_base  \n",
       "0         0.0        0.0  \n",
       "1         0.0        0.0  \n",
       "2         0.0        0.0  \n",
       "3         0.0        0.0  \n",
       "4         0.0        0.0  \n",
       "\n",
       "[5 rows x 5719 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "league_wrangled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest \n",
    "\n",
    "Random forest classifiers are made of an ensemble of decision trees. Decision trees work by seperating the data such that the homogeneity of the splits are maximized. These decision trees all return a classification, and the classification that appears most frequently is considered the final classification by the random forest. \n",
    "\n",
    "The random forest algorithm can easily be implemented using the Scikit-Learn Package. Because we are trying to predict between two classes (Blue team win, or Red team win), the RandomForestClassifier class will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (5079, 5719)\n",
      "Training Labels Shape: (5079,)\n",
      "Testing Features Shape: (2503, 5719)\n",
      "Testing Labels Shape: (2503,)\n"
     ]
    }
   ],
   "source": [
    "# First split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(league_wrangled, result, \n",
    "                                                    test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "print('Training Features Shape:', X_train.shape)\n",
    "print('Training Labels Shape:', y_train.shape)\n",
    "print('Testing Features Shape:', X_test.shape)\n",
    "print('Testing Labels Shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of random forest: 0.9684\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Random Forest \n",
    "rf_acc = rf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Mean accuracy of random forest: {rf_acc:0.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96      1090\n",
      "           1       0.98      0.97      0.97      1413\n",
      "\n",
      "    accuracy                           0.97      2503\n",
      "   macro avg       0.97      0.97      0.97      2503\n",
      "weighted avg       0.97      0.97      0.97      2503\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAAJbCAYAAABq/RV8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZhlZXUv/u9qRBEFJeJAAC+IWCqoCIgoohgjqD/HRAzeqDil4xTjkMSQmGgSSZyNU4zEWRONBr0OFxXU68QgMkmDUgiKiiAKoiAzzfv745yGoqmq3r3tU3Wa/fnw7KfOfvc+Z6/zPNSp1Wu9+z3VWgsAAKzLiuUOAACAjYPEEQCATiSOAAB0InEEAKATiSMAAJ1IHAEA6OQWyx0AG6W7J/nLJHsn2TXJN5Lst9Y5leSQJM9PsnWSbyd5cZJT5pzzzCTvn+f1n5/k38eP90vy/xaI48gkB6xn7MCUmZmZeXKSlyWZSXKbJD9K8uEkr5+dnb16Zmbmlkk+kmTPJNsk+U2SE5K8cnZ29sTliRqGSeJIH7skeUyS45LccoFz/jrJ32WUYJ6R0R+FL2WUaP5srXN/L8kVc/Z/MOfxSUketNb5d03y30k+3yN2YPrcIaN/IL4hya+S7JXk1UnukuRFSTZJ0pL8S5Kzk2yZ5KVJvjIzM3P/2dnZH8zzmsAElAXA6WFFkuvGj/8no4rifnOOb5bkgiRvSvKP47HbJDknybuTvHI89syMKo5bZFRB6OqvMvoDsn2S89YzdmAjMDMzc2iSFybZanZ29iZ/qGZmZm6b5KIkh8zOzr55qeODoTLHkT6uW8fxB2dUEfj4nLHLknw2yaM3wPUPSvK1SBrh5uyiLNzRSEafKVeu4xxgA9OqZhLumWR1ku+vNf69JH80z/lnZ9SqOjvJmzOqSi5k5yT3T7Lytw8TmCYzMzObJLlVkt0zmhP9rrnVxpmZmcqobb11kpdn9Dnz0WUIFQZL4sgkbJVR63n1WuMXJ9k8owrB1UnOz2ge5PEZ/TF4akY3xWye5C0LvPZTk1yT5PANHjWw3C7LKHFMkg9lNEd6rldkNE0lSX6R5DGzs7M/WqLYgGhVMznzTZ6ttY59MclrMro7+vNJnpFRe/uVWfj/zYPG5/9yg0UKTIsHJ9k3o2riE5K8Y63jH0jygCSPT3Jiks/NzMzceykDhKFTcWQSLs7ohpdNcuOq4+2TXJ5RxXAh/5PkKUl2yI3vrk6S+yW5V5JDN1SgwPSYnZ09afzwmzMzMxcm+eDMzMybZmdnzx4f/1nGqzLMzMx8PsnpGa3g8IzliBeGSMWRSTgjo6Tx7muN33N8rIv5KpYHZbRsz6f7hwZsJNYkkTvOd3B2dvbaJKuS3G3JIgIkjkzEMUkuSXLgnLHNkzwu61578Q+TXJjRAsBr+6OM7sxen6V7gI3TPuOfP5zv4MzMzGYZ3UQz73FgMrSq6WPzjBYAT5JtM1p658nj/SMyake/NqMbXy7ODQuAr0jy9jmvc3hGN8acmlGF8o/G24tz0yV/9s6o8vCyDftWgOU2MzPzhYy+IOD0jKa37JPRPMf/np2dPXtmZuapGS3l9YWMluHaJskLxj+t4QhLSOJIH3dK8om1xtbs75jRQt+vzShRPCSjpXZOSPLIjBYGX2M2ybMzWsi7knw3o7lKH57nmgcl+XV8WwzcHH07oy8E2CHJtRnNbz4kN3z16GySp2WUJG6V0YoM30qy5+zs7OlLHCsMmm+OAQCgE3McAQDoROIIAEAnEkcAADqROAIA0InEEQCATiSOAAB0InEEAKATiSMAAJ1IHAEA6ETiCABAJxJHAAA6kTgCANCJxBEAgE4kjgAAdCJxBACgE4kjAACdSBwBAOhE4ggAQCcSRwAAOpE4AgDQicQRAIBOJI4AAHQicQQAoBOJIwAAnUgcAQDoROIIAEAnEkcAADqROAIA0InEEQCATiSOAAB0InEEAKATiSMAAJ1IHAEA6ETiCABAJxJHAAA6kTgCANCJxBEAgE4kjgAAdCJxBACgE4kjAACdSBwBAOhE4ggAQCcSRwAAOpE4AgDQicQRAIBOJI4AAHQicQQAoBOJIwAAnUgcAQDoROIIAEAnEkcAADqROAIA0InEEQCATiSOAAB0InEEAKATiSMAAJ1IHAEA6ETiCABAJxJHAAA6kTgCANCJxBEAgE4kjgAAdCJxBACgE4kjAACdSBwBAOhE4ggAQCcSRwAAOpE4AgDQicQRAIBObrGuE6rq7klelmSHuee31vafXFgAAEybaq0tfkLVKUnem+TEJKvXjLfWvjXZ0AAAmCZdEseTWmu7L1E8AABMqS5zHD9dVSur6o5VteWabeKRAQAwVbpUHH8yz3Brrd11MiEBADCN1pk4AgBAsshd1VX1sNba16rq8fMdb619ZnJhAQAwbRZbjueRSb6W5MB5jrUkEkcAgAHRqgYAoJPFWtW/SHLMeDs6ybdba1ctVWAAAEyXBSuOVbVVkgclefB42y3JbMaJZGvtk0sVJAAAy69zq7qqNk/y7CQvSbJja22TSQYGAMB0WazieKfcUG3cK8mmSU5OclySY1trZy9VkAAALL/FEsfrkpyU5C1JDm+tXbmUgQEAMF0WSxz3zQ1zHLdPclaSY8fbSa21a5YqSAAAlt/6zHG8e5LHJHlxkm1ba7eeZGAAAEyXxRYAX5MsrpnnuE+SOyf5VpL3TD40AACmyWKt6guS/DJz1nJsrZ2xhLEBADBFFksc79Bau2iJ4wEAYEr5ykEAADpZsdwBAACwcZA4AgDQyaJ3VSdJVW2Z5GlJdph7fmvtZZMLCwCAabPOxDHJERl9g8yqJNdNNhwAAKbVOm+OqaqTWmu7L1E8AABMqS6J418kuSjJ55JctWa8tXbJZEMDAGCadEkcn5fkdUkuTbLm5NZau+uEYwMAYIp0SRzPTvKg1trPlyYkAACmUZfleL6bRFsaAGDgutxVfXWSk6vqK7nxHEfL8QAADEjX5XiOmHQgAABMN99VDQBAJ12+OWanJIcmuXeSzdaMt9buMcG4AACYMl1ujvlAkvcnqSSPTvLxJB+bYEwAAEyhLsvxnNha26OqVrXW7jMe+0Zrbd9FnrMyycok2Wq/F+xx210P2JAxAzdTZ77l8csdArCR2OwWqeWO4db3f9FE5vtdcfI7lv29LaRLxfGqqqokZ1fV86rqcUnutNgTWmuHtdb2bK3tKWkEALh56HJX9UuT3DbJizOa67hlkmdPMigAgKlXXepvNy/rTBxba99Kkqq6urX29MmHBADANFpnqlxVe1XVqiTfH+/fr6rePvHIAACmWdVktinWpcb6tiSPTXJRkrTWvpPk4ZMMCgCA6dNljuOK1tqP6sYZ8OoJxQMAsHEwx3FeP6mqvZK0qtokyZ8lOXOyYQEATLkpbytPQpdU+flJXpbkrkkuSLL3eAwAgAFZsOJYVS9qrb2jtfbzJActYUwAANNvgK3qxd6xtRoBALhelzmOAACsbYBzHBdLHO9bVZfMM15JWmttywnFBADAFFoscVzVWrv/kkUCALAxGeAcR61qAIA+BtiqXixV/sSSRQEAwNRbsOLYWvvnpQwEAGCjMsBW9fDeMQAAvZjjCADQxwDnOC72zTEvW+yJrbU3b/hwAAA2EgNsVS9Wcdxi/HMmyQOSfGa8/7gkX59kUAAATJ/Fbo75hySpqiOT7N5au3S8/+q44xoAGLoBtqq71FjvmuTqOftXJ9lhItEAADC1utwc8+Ekx1fVp5K0JE9K8qGJRgUAMO3Mcbyp1tqhVfX5JPuOh57VWjt5smEBAEy5ASaOXd/x5kkuaa29Ncm5VbXjBGMCAGAKrbPiWFWvSrJnRndXvz/Jpkk+kmSfyYYGADDFVrg5Zj5PSvL4JJclSWvtvNywVA8AAAPR5eaYq1trrapaklTVbSYcEwDA9DPHcV4fr6p3J7l9Vf1Jki8lec9kwwIAYNp0uav6jVX1yCSXZDTP8e9ba0dNPDIAgGk2wAXAu7SqM04Uj0qSqtqkqv64tfafE40MAGCaaVXfoKq2rKpDquodVbV/jbwoyQ+SPGXpQgQAYBosVnH8cJKLkxyb5LlJ/jLJLZM8obV2yhLEBgAwvbSqb+RurbX7JElVvSfJhUnu2lq7dEkiAwBgqizWnL9mzYPW2uokP5Q0AgCM1YrJbOu6bNX7qurnVXXanLE3VNUZVXVqVX2qqm4/Ht+hqq6oqlPG27/Pec4eVbWqqs6qqrdVrbuEulh096uqS8bbpUnuu+ZxVV2yzncFAHBzVjWZbd0+kORRa40dlWTX1tp9k5yZ5JA5x85ure023p43Z/xdSVYm2Xm8rf2aN7Fg4tha26S1tuV426K1dos5j7fs8q4AANiwWmtfT/LLtcaObK1dO949Lsl2i71GVW2TZMvW2rGttZbkQ0meuK5rD+8+cgCADWFCreqqWllVJ8zZVq5nZM9O8vk5+ztW1clV9bWq2nc8tm2Sc+ecc+54bFGd1nEEAGBptNYOS3JYn+dW1d8muTbJmvW2z8/o5uaLqmqPJP+nqnZJMl9PvK3r9SWOAAB9TNlyPFV1cJLHJnnEuP2c1tpVSa4aPz6xqs5Oco+MKoxz29nbJTlvXdfQqgYA6GOZ7qqeN5SqRyV5RZLHt9YunzN+x6raZPz4bhndBPOD1tr5SS6tqr3Hd1M/I8mn13UdFUcAgI1IVX00yX5Jtq6qc5O8KqO7qG+V5KjxqjrHje+gfmiSf6yqa5OsTvK81tqaG2uen9Ed2rfOaE7k3HmR85I4AgD0sUyt6tbaU+cZfu8C5x6e5PAFjp2QZNf1ubZWNQAAnag4AgD00XM+4sZseO8YAIBeVBwBAPoYYMVR4ggA0MeUreO4FIaXKgMA0IuKIwBAHwNsVQ/vHQMA0IuKIwBAHwOc4yhxBADoQ6saAADmp+IIANDHAFvVKo4AAHSi4ggA0EMNsOIocQQA6GGIiaNWNQAAnag4AgD0MbyCo4ojAADdqDgCAPRgjiMAACxAxREAoIchVhwljgAAPQwxcdSqBgCgExVHAIAeVBwBAGABKo4AAH0Mr+AocQQA6EOrGgAAFqDiCADQg4ojAAAsQMURAKCHIVYcJY4AAD0MMXHUqgYAoBMVRwCAPoZXcFRxBACgGxVHAIAezHEEAIAFqDgCAPQwxIqjxBEAoIchJo5a1QAAdKLiCADQx/AKjiqOAAB0o+IIANDDEOc4ShwBAHoYYuKoVQ0AQCcqjgAAPag4AgDAAlQcAQB6GGLFUeIIANDH8PJGrWoAALpRcQQA6GGIrWoVRwAAOlFxBADoQcURAAAWoOIIANDDECuOEkcAgD6GlzdqVQMA0I2KIwBAD0NsVas4AgDQiYojAEAPQ6w4ShwBAHoYYuKoVQ0AQCcqjgAAPag4AgDAAiZecTzjzY+b9CWAm4mtHvCi5Q4B2EhccfI7ljuEQS4ArlUNANCDVjUAACxAxREAoAcVRwAAWICKIwBADwMsOEocAQD60KoGAIAFSBwBAHqomsy27uvW+6rq51V12pyx36mqo6rq++OfW43Hq6reVlVnVdWpVbX7nOccPD7/+1V1cJf3LHEEANi4fCDJo9Ya++skX26t7Zzky+P9JHl0kp3H28ok70pGiWaSVyV5YJK9krxqTbK5GIkjAEAPVTWRbV1aa19P8su1hp+Q5IPjxx9M8sQ54x9qI8cluX1VbZPkgCRHtdZ+2Vq7OMlRuWkyehMSRwCAjd+dW2vnJ8n4553G49sm+cmc884djy00vih3VQMA9DCpm6qramVGbeU1DmutHdb35eYZa4uML0riCADQw4oVk8kcx0ni+iaKF1TVNq2188et6J+Px89Nsv2c87ZLct54fL+1xr+6rotoVQMAbPw+k2TNndEHJ/n0nPFnjO+u3jvJr8et7C8m2b+qthrfFLP/eGxRKo4AAD0s1/rfVfXRjKqFW1fVuRndHf3aJB+vquck+XGSA8enH5HkMUnOSnJ5kmclSWvtl1X1T0m+PT7vH1tra99wcxMSRwCAjUhr7akLHHrEPOe2JC9c4HXel+R963NtiSMAQA9D/MpBiSMAQA8DzBvdHAMAQDcqjgAAPQyxVa3iCABAJyqOAAA9DLHiKHEEAOhhgHmjVjUAAN2oOAIA9DDEVrWKIwAAnag4AgD0MMCCo4ojAADdqDgCAPQwxDmOEkcAgB4GmDdqVQMA0I2KIwBAD0NsVas4AgDQiYojAEAPAyw4ShwBAPrQqgYAgAWoOAIA9DDAgqOKIwAA3ag4AgD0MMQ5jhJHAIAeBpg3alUDANCNiiMAQA9DbFWrOAIA0ImKIwBADwMsOKo4AgDQjYojAEAPQ5zjKHEEAOhhiImjVjUAAJ2oOAIA9DDAgqOKIwAA3ag4AgD0MMQ5jhJHAIAeBpg3alUDANCNiiMAQA9DbFWrOAIA0ImKIwBADwMsOEocAQD6WDHAzFGrGgCATlQcAQB6GGDBUcURAIBuVBwBAHqwHA8AACxAxREAoIcVwys4ShwBAPrQqgYAgAWoOAIA9DDAgqOKIwAA3ag4AgD0UBleyVHiCADQwxDvqtaqBgCgExVHAIAeLMcDAAALUHEEAOhhgAVHiSMAQB8rBpg5alUDANCJiiMAQA8DLDiqOAIA0I2KIwBAD5bjAQCABag4AgD0MMCCo8QRAKAPy/EAAMACVBwBAHoYXr1RxREAgI5UHAEAehjicjwSRwCAHlYML2/UqgYAoBsVRwCAHobYqlZxBACgE4kjAEAPVZPZ1n3dmqmqU+Zsl1TVS6rq1VX10znjj5nznEOq6qyqmq2qA/q+Z61qAIAelqtV3VqbTbLbOIZNkvw0yaeSPCvJW1prb5x7flXdO8lBSXZJ8rtJvlRV92itrV7fa6s4AgBsvB6R5OzW2o8WOecJST7WWruqtfbDJGcl2avPxSSOAAA9rKjJbFW1sqpOmLOtXCSMg5J8dM7+i6rq1Kp6X1VtNR7bNslP5pxz7nhs/d9znycBADAZrbXDWmt7ztkOm++8qrplkscn+cR46F1JdsqojX1+kjetOXW+y/SJzRxHAIAepmA5nkcnOam1dkGSrPmZJFX1H0k+N949N8n2c563XZLz+lxQxREAYOP01MxpU1fVNnOOPSnJaePHn0lyUFXdqqp2TLJzkuP7XFDFEQCgh+WsN1bV5kkemeRP5wy/vqp2y6gNfc6aY62106vq40m+m+TaJC/sc0d1InEEAOhlxTK2qltrlye5w1pjT1/k/EOTHPrbXlerGgCATlQcAQB6WP57Y5aeiiMAAJ2oOAIA9DAFy/EsOYkjAEAPA8wbtaoBAOhGxREAoIflXI5nuag4AgDQiYojAEAPAyw4ShwBAPoY4l3VWtUAAHSi4ggA0MMQq29DfM8AAPSg4ggA0IM5jgAAsAAVRwCAHlYMr+AocQQA6GOIiaNWNQAAnag4AgD04OYYAABYgIojAEAPQ5zjKHEEAOhhgJ1qrWoAALpRcQQA6GHFAEuOKo4AAHSi4ggA0MMQq28SRwCAHgbYqR5ksgwAQA8qjgAAPbg5BgAAFqDiCADQwwALjiqOAAB0o+IIANCD76oGAKATN8cAAMACVBwBAHoYYMFRxREAgG5UHAEAenBzDAAAnVSGlzlqVQMA0ImKIwBAD0NsVas4AgDQiYojAEAPQ6w4ShwBAHqoAS7kqFUNAEAnKo4AAD0MsVWt4ggAQCcqjgAAPQxwiqPEEQCgjxUDzBy1qgEA6ETFEQCgBzfHAADAAlQcAQB6GOAURxVHAAC6UXEEAOhhRYZXcpQ4AgD0oFUNAAALUHEEAOjBcjwAALAAFUcAgB6G+JWDEkcAgB4GmDdqVQMA0I2KIwBAD0NsVas4AgDQiYojAEAPAyw4ShwBAPoYYtt2iO8ZAIAeVBwBAHqoAfaqVRwBAOhExREAoIfh1RtVHAEA6EjFEQCgBwuAAwDQSU1o63TtqnOqalVVnVJVJ4zHfqeqjqqq749/bjUer6p6W1WdVVWnVtXufd+zxBEAYOP08Nbabq21Pcf7f53ky621nZN8ebyfJI9OsvN4W5nkXX0vKHEEAOihajLbb+EJST44fvzBJE+cM/6hNnJckttX1TZ9LiBxBADY+LQkR1bViVW1cjx259ba+Uky/nmn8fi2SX4y57nnjsfWm5tjAAB6mNQC4ONEcOWcocNaa4etddo+rbXzqupOSY6qqjMWe8l5xlqf2CSOAAA9TKptO04S104U1z7nvPHPn1fVp5LsleSCqtqmtXb+uBX98/Hp5ybZfs7Tt0tyXp/YtKoBADYiVXWbqtpizeMk+yc5Lclnkhw8Pu3gJJ8eP/5MkmeM767eO8mv17S015eKIwBAD8v4XdV3TvKp8fVvkeS/WmtfqKpvJ/l4VT0nyY+THDg+/4gkj0lyVpLLkzyr74UljgAAG5HW2g+S3G+e8YuSPGKe8ZbkhRvi2hJHAIAehve9MRJHAIBelrFVvWzcHAMAQCcqjgAAPQyx+jbE9wwAQA8qjgAAPZjjCAAAC1BxBADoYXj1RokjAEAvA+xUa1UDANCNiiMAQA8rBtisVnEEAKATFUcAgB6GOMdR4ggA0ENpVQMAwPxUHAEAehhiq1rFEQCATlQcAQB6GOJyPBJHAIAetKoBAGABKo4AAD2oOAIAwAJUHAEAerAAOAAALEDFEQCghxXDKzhKHAEA+tCqBgCABag4AgD0YDkeAABYgIojAEAPQ5zjKHEEAOjBXdUwAT+/4II88bGPzhVXXJ6jjz8xm29+m+uPff/M2bz9X9+ck086Mdddd112vNtO+Zu/e1XuvcuuyxgxsKHcbfut89Jn/H72uu8O2WWn383RJ5+dA/7krdcf3/QWm+T9hx6c3e9919xl6y3zmyuuyknf/XH+4Z2fy8nf+8mNXmuTTVbkpc94RA5+4oOy/V22yoUX/yafPOrk/NWbPpkk2XePnXPke/583jiOOuZ7efwL3zm5NwoDIXFk4t7yptdn8803zxVXXH6j8dkzvpdnP+OPs9/DH5HXvvEtSZLTT1uVq666ajnCBCbg3jttk0c9ZJccv+qHueUtbvonZ5NNVqS1lje878j84NwLs+VtNsufPe3h+fy7X5y9n/ranPPTi64/97BXPy0Pf+BMDn33EZk954Jsd+etcq+73eX646ec8ZM87BlvvNHrb3+XrfKR1z8nRx59+uTeJIOlVQ0b2EknnpBjvvnNPOdPVuYtb3rDjY4d+o+vzkP3e3gOfd0N4/s8ZN+lDhGYoP/7tdPyua+uSpL81xuekzvc/rY3On7lVdfk6X/9/huNfeVbZ+SnX31dHv/w++VtH/lKkuSRD75XDjxgj+x10L/kjB/8bN5rXXrZlTl+1Tk3GnvI7nfP6tXX5fCjTt5A7wiGzV3VTMzq1avzun9+TVY+/wW5/VZb3ejY2WeflVWnficH/e+nLVN0wFJora33cy674upcedW1ueWmm1w/dvATHpSvfvvMBZPGhTz5gD3yjRO/n/N/8ev1jgPWpWoy2zSTODIx//Pxj+Xqq6/KUw763zc5dtqp30mSXHLJJXnKHzwhe95vlzzuUY/Mpw7/n6UOE5gSm2yyIne+wxb555c8Mauvuy4f/8IJ1x97wH12yFk//nne8ooDc8E33pCLjnlzPvbG52abO95uwdfb6a53zP3vtX0+/oUTlyJ8BqgmtE0zrWom4le/ujj/9va35TWvfX023XTTmxy/8MILkyR//zevyMHPfm522XXXfOnII/OPr3pltr7jHbPvQx+21CEDy+gvnvXI/NOLn5Ak+fkvL82T/uxd+fH5F19//M532CJPe9wDs+rMn+YZh7w/W2y+WQ59yRPz32/6kzx0rXmNazzlUXvk6muuzf/58ilL8h5gCCSOTMQ73vqv2fW+910wAWzXjdpXT/zDA/PMZz83SfKAvfbOD39wdt7/nsMkjjAwH/7McfnKt2Zzl623zMqn7JvD3/q8PPK5/3p9a7qqUlU58KWH5Ze/vixJcv6Fv86X3vvS7LfXPfLV48+8yWseeMAe+dKxZ+TiSy6/yTHYEFZMe195ArSq2eDOPuv7+fSnPpk/ff4Lc+kll+TSSy7JlVdemST5zaW/yZVXXpktbzdqLz1gr71u9NwHPHDv/ODss5Y8ZmB5XXDRpTnpuz/OEV8/LX/45+/OL399Wf7iWY+8/vjFl1ye08867/qkMUmOOfkHuerqa3Kvu21zk9e7zz22zb3utk0+MafdDfz2VBzZ4H78ox/l2muvycF/fNBNjh3wiIfliX/w5Px/j33cvM9traVW+PcMDNnq1dfl9LPOy47bbn392OwPL8itbnnTP1lVleuuu+kNOAcesEcuv+LqfParp040VoZtePVGiSMTsNvue+Q/3vfBG40dffQ384H3/kfe/q7Dst1222Xb7bbLllveLscfd1wevM8NS/Acf9yxucfMPZc6ZGCK3OqWt8hu99w+x57yg+vHPv/10/LK5z8md7j9bXLRr0ZVx4fsfvfcctNbZNWZ597kNZ68/+454uurctkVVy9Z3DAEEkc2uK222ip77vXAG42dd95PkyS777HH9d8cs/L5L8i/vumN2WKLLbLLrvfJl446MiedeELe84EPL3nMwGTcerNN86iH7JIk+d073T5b3GazPOn3d0uSfOGbp+dx+903++9z7xx19Pdy3i9+nW3uuGVWHrhv7rL1ltev4Zgk7/3k0XnBUx+Ww9/6vLz+vV/MFptvltf8+RPy5ePOyDFzEswk2es+O2TH7bbOK8bfKAMTM8CSo8SRZfPHTz841113XT72Xx/Jv//bO7PDjjvkDW9+a3bfY8/lDg3YQO641Rb5rzc890Zja/ZnHvP3OfOcC3LQYx6Q1778D7LVlrfOzy68JN9edU72+ePX53tz1my89LIr86g/fVve9FcH5kOvfVauvmZ1PvfVU/NXbzz8Jtc88IA98qtLL88Xj/7uZN8cgzfEb46pPouzro/Lr5nwBYCbjTvs9WfLHQKwkbji5Hcse9b2rbN/PZEc54E73W7Z39tCVBwBAHoY4Go8luMBAKAbFUcAgB4GWHCUOAIA9DLAzFGrGufwaiUAAA7TSURBVACATlQcAQB6GOJyPCqOAAB0ouIIANDDEJfjkTgCAPQwwLxRqxoAgG5UHAEA+hhgyVHFEQCATlQcAQB6sBwPAAAsQMURAKAHy/EAANDJAPNGrWoAALpRcQQA6GOAJUcVRwAAOlFxBADoYYjL8UgcAQB6GOJd1VrVAAB0ouIIANDDAAuOKo4AAHSj4ggA0McAS44SRwCAHoZ4V7VWNQAAnag4AgD0YDkeAABYgMQRAKCHmtC2zutWbV9V/6+qvldVp1fVn4/HX11VP62qU8bbY+Y855CqOquqZqvqgL7vWasaAGDjcm2Sl7fWTqqqLZKcWFVHjY+9pbX2xrknV9W9kxyUZJckv5vkS1V1j9ba6vW9sIojAEAfy1RybK2d31o7afz40iTfS7LtIk95QpKPtdauaq39MMlZSfZar/c6JnEEAOihJvVf1cqqOmHOtnLBGKp2SHL/JN8aD72oqk6tqvdV1VbjsW2T/GTO087N4onmgiSOAABTpLV2WGttzznbYfOdV1W3TXJ4kpe01i5J8q4kOyXZLcn5Sd605tT5LtMnNnMcAQB6WM7leKpq04ySxv9srX0ySVprF8w5/h9JPjfePTfJ9nOevl2S8/pcV8URAGAjUlWV5L1Jvtdae/Oc8W3mnPakJKeNH38myUFVdauq2jHJzkmO73NtFUcAgB6WseC4T5KnJ1lVVaeMx/4myVOrareM2tDnJPnTJGmtnV5VH0/y3YzuyH5hnzuqE4kjAEA/y5Q5tta+ucDVj1jkOYcmOfS3vbZWNQAAnag4AgD0UMvZrF4mKo4AAHSi4ggA0MNyLsezXCSOAAA9DDBv1KoGAKAbFUcAgD4GWHJUcQQAoBMVRwCAHoa4HI/EEQCghyHeVa1VDQBAJyqOAAA9DLDgqOIIAEA3Ko4AAH0MsOSo4ggAQCcqjgAAPViOBwCATizHAwAAC1BxBADoYYAFRxVHAAC6UXEEAOhhiHMcJY4AAL0ML3PUqgYAoBMVRwCAHobYqlZxBACgExVHAIAeBlhwlDgCAPShVQ0AAAtQcQQA6KEG2KxeZ+JYVVsneXaSHeae31pbObmwAACYNl0qjp9OclySbyZZPdlwAAA2EsMrOHZKHG/TWnv5xCMBAGCqdbk55vNVtf/EIwEA2IjUhLZp1qXi+Lwkr6iqy5NcndF7aq2135loZAAAU2yIy/F0SRy3nngUAABMvQUTx6raubX2/SS7LHDKqZMJCQBg+lmO58YOyWgZnnfOc6wleehEIgIAYCotmDi21p49/rnv0oUDALCRGF7BcdFW9YkZrd14TJKjW2vnLllUAABTboB546LL8TwnyWySxyX5elX9uKo+WlV/VlV7LE14AABMi8Va1ackOSXJvyVJVd05yYFJXpLkX5NsshQBAgBMI8vxzFFVleR+SR6cZJ8k90hyQZIPJDl2KYIDAGB6LHZX9SUZtarfleTV46V5AACI5XjW9oIkD0ry3CRPq6rjM6o0Httau2ApggMAmFZa1XO01j6c5MNJUlW3TbJ3Rm3rN1TVitbaTksTIgAA02DRrxysqs2S7JUb5jk+MKN5jkdPPjQAAKbJYjfHfDvJTklOyqhF/c4kT2ut/XqJYgMAYIosVnH80yTfaa2tXqpgAAA2FuY4ztFaO2kpAwEAYLotOscRAID5WY4HAIBOtKrnUVUrkjwqyQ5zz2+tvW1yYQEAMG26VBw/naQlWZXkusmGAwCwcRhgwbFT4rhDa+0+E48EAICptqLDOV+sqt+beCQAABuTmtA2xbpUHL+R5LNV1ZJcndFbaq2135loZAAAU8xd1fN7S5J9Y44jAMCgdUkcv5/k5NZam3QwAAAbC8vxzO+8JF+pqiOSXLVm0HI8AADD0iVxPHe8bTnhWAAANhoDLDiuO3Fsrf3dUgQCALBRGWDm2OWbY7ZO8vIkuyTZbM14a23/CcYFAMCU6bKO40eSnJPkHklel+RnSU6ZYEwAAFOvJvTfNOuSON6xtfbuJFe31r6c5OAke002LAAApk2Xm2OuGf/8WVUdkNFd1ttPLiQAgOk3xOV4al3LM1bV45N8Lcn/SvLOjO6u/ofW2icXec7KJCvHu4e11g7bMOFyc1FVK/1/AXTh8wKmxzoTR5iEqjqhtbbncscBTD+fFzA91jnHsaruXlVfrKrvjPfvW1WHTD40AACmSZebY96T5B9yw/dUr0rytIlFBADAVOqSON6mtXbMmp3xd1Zfs8j50IX5SkBXPi9gSnRJHC+qqh2TtCSpqidmtJYj9GaiO9CVzwuYHl3uqr57Rv/a2zvJL5Kcn+Sg1to5E48OAICpsWDFsar+IElaa2e11n4vyTZJ7tda21vSuHGqqtVVdUpVnVZVn62q26/n819dVX+x1tjfjl/zlDmvf0pVvXjDRj9vPFtX1c/n7O9bVa2q7jLe/52qurBGDq2qh086JtjYzfk9/k5VnVRVDx6P71BVp22A13/n+PW/W1VXzPnMePJvH/06r71HVZ0wZ//pVfWbqtpkvH//qjpp/Pj9VTUz6ZhgY7PYAuCvTHL9Wo2ttV9PPhwm7IrW2m5JUlUfTPLCJIf+Ni/YWjt0zWtU1W/WvP5SaK1dWFUXV9U9WmtnJnlwkpPHPz+Z5EFJjh3Py/3bpYoLNnJzPycOSPIvSR62oV68tfbC8WvvkORzS/mZkeQ7Se5eVZu31i7P6LPizCT3S3LSeP/ocZzPWsK4YKPRZY4jN0/HJtl2zU5V/WVVfbuqTq2qf5gz/rdVNVtVX0qyXv/6rqo7V9Unq+qEqjq+qvYej+9dVcdW1clVdXRV7Twef+74/M9V1Q+r6vnjuE6uqmMWqJAendGHfcY/37LW/jHj1/7IeH5uqurccfX05PH7vcf6vC8YkC2TXLz2YFU9s6reMWf/c1W13/jx/uPf75Oq6hNVdduuF6uqncfLv51YVV9f87tZVU+oqm+Nf2ePrKo7jcdfU1UfGI+dU1VPrKo3jbsq/7eqblQcaa1dm1GCuOZrc++f5F2Z/zPjm1W1W1Xdoqp+VVWvHVdhj11zfRiixRLHe47/qK69raqqU5csQja4cVvmEUk+M97fP8nOGX2Y7pZkj6p6aFXtkeSgjD5c/yDJA9bzUm9L8vrxwr1PyWhppyT5XpKHtNbun+SfkrxmznN2SfJHGc2pfV2Si8fnnZj5l4E6Jjd86N81yeFz4ry+ejCPC8av+54kL1vP9wU3Z7cet47PyOj345+6PrGqts6oW/X7rbXdk5yQ9fv9OizJC1preyQ5JMma5PTrSfYe/85+MsnL5zxnxySPSfKHSf4ryRdaa7tmtITco+a5xjFJHlxVWyS5avzacxPH+T4zbpfka621+2X0j+5nr8d7gpuVxVrVP0zyuKUKhCVx66o6JckOGSViR43H9x9vJ4/3b5tRIrlFkk+NWzqpqs+s5/V+P8lM3fBlnltV1a2T3D7Jh6pqp3me85XW2mVJLquq3yT57Hh8VZL5KoNHJ3nJuGp5Vmvt8qq6ZVXdJqMk+NsLxLZmGsaJGf3RAUbmtqoflNHv6q4dn7t3knsnOXr8e3/LjBKtdRp3FPZOcvicz4w1f6PumuTjNZq/fKuM2strHNFau7aqViVJa23N59qqjD7r1nZ0RtN0vp3k+NbabFXNjF9709baj+d5zhWttc+PH5+YZN8u7wlujhZLHK9urf1oySJhKVzRWtutqm6X5HMZfXi+LUkl+ZfW2rvnnlxVL8l4GaaeKslerbWr13rdQ5N8sbX2bzW6a/8Lcw5fNefxdXP2r8v8/7+ekeTOGSV/a/5AnZxRReDM1toVC8S25nVXL/C6MHittWPHVcQ7rnXo2ty4Y7XZ+GclOaq19tQel6skFy4w5/GdSf65tXZEVf1+kr+ec2zuZ8Tcz5qFPjOOTfKhJPvkhs+MnyU5MAt3KOa+rs8MBm2xVvVCv0Bs5MY3Or04yV9U1aZJvpjk2WvmIlXVtuM5PF9P8qSquvW4rbO+FegvZZScZvy6a/4g3C7JT8ePn9n7jeT6Bem/ldH7WfNH4NgkL8l4rhLQT1XdM8kmSS5a69A5SXarqhVVtX1umDN4XJJ9xv8gTFVt3nUOcWvt4iTnV9WTxs9dUVX3Gx++XZKf1qgUefBv855aa79KckGSp+eGz4zj4jMDOlkwcWytvWgpA2FptdZOzugOw4Naa0dmNDfo2HG753+SbNFaOynJfyc5JaO5g99Yz8u8MKM/IqdW1XeT/Ml4/HVJ3lBVG+ofJ0cn2S6jSe/J6I/B3eKPAPSxZo7jKRn9/h/cWlu91jlHZzSdaVWSN2b8u9da+0VG/xj86Hgu/HFJ7rke1z4oyfOq6jtJTk/y2PH4q5N8KsnXMkr6fltHJ9mktXb+eN9nBnS0zgXAAQAgsRwPAAAdrTNxHM9R+buq+o/x/s5V9dh1PQ8AgJuXLhXH92d019qDxvvn5sbr7gEAMABdEsedWmuvT3JNkoyXN6nFnwIAwM1Nl8Tx6vGizS1Jxos2X7X4UwAAuLnpsojpqzJaoHn7qvrPjBZNfeYkgwIAYPp0Wo6nqu6Q0VdBVZLjWmsXTjowAACmyzoTx6p66HzjrbWvTyQiAACmUpfE8bNzdjfL6KulTmyt/d4kAwMAYLqsc45ja+1G3088/l7S108sIgAAplKfb445N8muGzoQAACm2zorjlX19oyX4sko0dwtyXcmGRQAANOnyxzHg+fsXpvknNba0RONCgCAqdNpOR4AAFiwVV1Vq3JDi/pGh5K01tp9JxYVAABTZ8GKY1X9r8We2Fr70UQiAgBgKq1Xq7qqtk5yUdPfBgAYnAWX46mqvavqq1X1yaq6f1WdluS0JBdU1aOWLkQAAKbBYq3qE5L8TZLbJTksyaNba8dV1T2TfLS1dv+lCxMAgOW22ALgt2itHdla+0SSn7XWjkuS1toZSxMaAADTZLHE8bo5j69Y65g5jgAAA7NYq3p1kssyWn7n1kkuX3MoyWattU2XJEIAAKaCBcABAOhksVY1AABcT+IIAEAnEkcAADqROAIA0InEEQCATiSOAAB08v8DI7UeiDZmZ2sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display classification report and confusino matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test, predictions)\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.heatmap(conf_mat, annot=True, annot_kws={'size':15}, fmt='g', cbar=False, cmap=\"Blues\",\n",
    "            xticklabels=['Red Team Win', 'Blue Team Win'],  yticklabels=['Red Team Win', 'Blue Team Win'])\n",
    "\n",
    "# ax.set_title(\"Confusion Matrix\", size=15)\n",
    "plt.yticks([0.75,1.75], ['Red Team Win', 'Blue Team Win'], va='center');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "In order to see which features were the most influential in determining who would win, the variable importances will be inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: bInhibs              Importance: 0.0745\n",
      "Variable: rInhibs              Importance: 0.06558\n",
      "Variable: rmiddle_inner        Importance: 0.04422\n",
      "Variable: bbmiddle_base        Importance: 0.04298\n",
      "Variable: rbmiddle_base        Importance: 0.04043\n",
      "Variable: bmiddle_inner        Importance: 0.04002\n",
      "Variable: rKills               Importance: 0.0329\n",
      "Variable: rBarons              Importance: 0.03133\n",
      "Variable: rbot_inner           Importance: 0.02718\n",
      "Variable: bKills               Importance: 0.02693\n",
      "Variable: bBarons              Importance: 0.02635\n",
      "Variable: rtop_inner           Importance: 0.02596\n",
      "Variable: bbot_inner           Importance: 0.02577\n",
      "Variable: btop_inner           Importance: 0.02484\n",
      "Variable: rbot_base            Importance: 0.02364\n",
      "Variable: bbot_base            Importance: 0.0204\n",
      "Variable: num_of_drags         Importance: 0.01985\n",
      "Variable: rtop_base            Importance: 0.01844\n",
      "Variable: btop_base            Importance: 0.01509\n",
      "Variable: rmiddle_outer        Importance: 0.01455\n",
      "Variable: goldredSupport       Importance: 0.01453\n",
      "Variable: goldredADC           Importance: 0.01144\n",
      "Variable: bmiddle_outer        Importance: 0.01116\n",
      "Variable: goldred              Importance: 0.01106\n",
      "Variable: goldblue             Importance: 0.01077\n",
      "Variable: goldredJungle        Importance: 0.01031\n",
      "Variable: rtop_outer           Importance: 0.00955\n",
      "Variable: goldblueSupport      Importance: 0.00919\n",
      "Variable: goldblueMiddle       Importance: 0.00871\n",
      "Variable: goldblueJungle       Importance: 0.00848\n",
      "Variable: goldredMiddle        Importance: 0.00846\n",
      "Variable: goldblueADC          Importance: 0.00836\n",
      "Variable: goldredTop           Importance: 0.00818\n",
      "Variable: rbot_outer           Importance: 0.00763\n",
      "Variable: btop_outer           Importance: 0.00756\n"
     ]
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 5)) for feature, importance in zip(league_wrangled.columns, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print(f'Variable: {pair[0]:20} Importance: {pair[1]}') for pair in feature_importances[:35]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the feature importances, it is clear that the most important features are the number of inhibtors taken by each team. Afterwards, the inner and base towers are the most significant. Based on the values, it is clear that games are not decided by a single feature alone, but rather a combination of multiple features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppot Vector Machine (SVM)\n",
    "\n",
    "A SVM is a classifier that seperates the data points using hyperplanes. Hyperplanes are calculated by maximizing the distance between support vectors and the hyperplane. SVMs take advantage of the kernel trick in order to model nonlinear features. Their advantages include: \n",
    "\n",
    "- Performs well for high-dimensional data\n",
    "- Useful when classes are seperable \n",
    "- Suited for binary classification\n",
    "\n",
    "All of these advantages are applicable to the wrangled league of legends dataset. With regards to the SVM's disadvantages, these include: \n",
    "\n",
    "- Requires a large amount of processing time for large datasets\n",
    "- Does not perform well on overlapping classes\n",
    "- Can be difficult selecting the proper kernel function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "league_ss = league_wrangled.copy()\n",
    "ss = StandardScaler() \n",
    "league_ss = ss.fit_transform(league_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train_ss, X_test_ss, y_train_ss, y_test_ss = train_test_split(league_ss, result, \n",
    "                                                    test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(gamma=\"scale\") \n",
    "svc.fit(X_train_ss, y_train_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy of SVC: 0.9372752696763883\n"
     ]
    }
   ],
   "source": [
    "svc_acc = svc.score(X_test_ss, y_test_ss)\n",
    "\n",
    "print(f\"Mean Accuracy of SVC: {svc_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network \n",
    "\n",
    "Neural networks are modeled after the way neurons are structured. They are able to learn complex relationships and functions through the adjustment of weights by gradient descent. Simply put, they learn by reducing the error between the predicted value/class and the actual value/class.\n",
    "\n",
    "Because of the nature of neural networks, a simple neural network will be constructed using a single hidden layer. The layer will have half the number of features, will a final single output neuron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform inputs \n",
    "X_train_array = np.asarray(X_train_ss)\n",
    "y_train_array = np.asarray(y_train_ss)\n",
    "\n",
    "X_test_array = np.asarray(X_test_ss)\n",
    "y_test_array = np.asarray(y_test_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "\n",
    "input_size = len(X_train_array[0])\n",
    "inputs = keras.Input(shape=(input_size, )) \n",
    "dense = layers.Dense(0.5*input_size, activation='relu')(inputs)\n",
    "output = layers.Dense(1, activation='softmax')(dense)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=output, name='base_league_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"base_league_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 5719)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2859)              16353480  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2860      \n",
      "=================================================================\n",
      "Total params: 16,356,340\n",
      "Trainable params: 16,356,340\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5079 samples\n",
      "Epoch 1/100\n",
      "5079/5079 [==============================] - 1s 248us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 2/100\n",
      "5079/5079 [==============================] - 0s 97us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 3/100\n",
      "5079/5079 [==============================] - 1s 99us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 4/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344 - loss: 7.2141 \n",
      "Epoch 5/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 6/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 7/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 8/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 9/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 10/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 11/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 12/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 13/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 14/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 15/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 16/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 17/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 18/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 19/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 20/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 21/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 22/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 23/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 24/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 25/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 26/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 27/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 28/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 29/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 30/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 31/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 32/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 33/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 34/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 35/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 36/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 37/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 38/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 39/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 40/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 41/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 42/100\n",
      "5079/5079 [==============================] - 0s 97us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 43/100\n",
      "5079/5079 [==============================] - 0s 96us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 44/100\n",
      "5079/5079 [==============================] - 0s 97us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 45/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 46/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 47/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 48/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 49/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 50/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 51/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 52/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 53/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 54/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 55/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 56/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 57/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 58/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 59/100\n",
      "5079/5079 [==============================] - 1s 102us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 60/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 61/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 62/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 63/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 64/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 65/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 66/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 67/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 68/100\n",
      "5079/5079 [==============================] - ETA: 0s - loss: 7.1251 - accuracy: 0.53 - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 69/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 70/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 71/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 72/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 73/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 74/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 75/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 77/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 78/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 79/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 80/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 81/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 82/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 83/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 84/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 85/100\n",
      "5079/5079 [==============================] - 1s 102us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 86/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 87/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 88/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 89/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 90/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 91/100\n",
      "5079/5079 [==============================] - 1s 100us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 92/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 93/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 94/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 95/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 96/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 97/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 98/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 99/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n",
      "Epoch 100/100\n",
      "5079/5079 [==============================] - 1s 101us/sample - loss: 7.1398 - accuracy: 0.5344\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2033de21588>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_array, y_train_array, \n",
    "          batch_size=64, \n",
    "          epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2503/1 - 0s - loss: 6.6243 - accuracy: 0.5645\n",
      "Test loss: 6.677279455205511\n",
      "Test accuracy: 0.56452256\n"
     ]
    }
   ],
   "source": [
    "test_scores = model.evaluate(X_test_array, y_test_array, verbose=2)\n",
    "\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the neural network did not perform as well as the other two models with an accuracy of 56%. This is likely because a single hidden layer is not enough to properly capture the underlying function governing the result of the games. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Models\n",
    "\n",
    "Despite the random forest and svm classifiers' already high performance, gridsearchCV will be used in an attempt to improve their accuracies. GridsearchCV will also be applied to the neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf_parameters = {\n",
    "    'n_estimators':[100,200,500,1000],\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'max_features':['auto', 'sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "svc_parameters = {\n",
    "    'C':[0.001, 0.01, 0.1, 1, 10],\n",
    "    'gamma': ['auto', 'scale']\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='warn', n_jobs=2,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_features': ['auto', 'sqrt', 'log2', None],\n",
       "                         'n_estimators': [100, 200, 500, 1000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf_clf = GridSearchCV(rf, rf_parameters, cv=3, n_jobs=2)\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest parameters\n",
      "{'criterion': 'entropy', 'max_features': None, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Random Forest parameters\")\n",
    "print(rf_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of random forest: 0.9808\n"
     ]
    }
   ],
   "source": [
    "rf_clf_acc = rf_clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Mean accuracy of random forest: {rf_clf_acc:0.4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='warn', n_jobs=2,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10],\n",
       "                         'gamma': ['auto', 'scale']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC() \n",
    "svc_clf = GridSearchCV(svc, svc_parameters, cv=3, n_jobs=2)\n",
    "svc_clf.fit(X_train_ss, y_train_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVC parameters\n",
      "{'C': 10, 'gamma': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best SVC parameters\")\n",
    "print(svc_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy of SVC: 0.9368757491010787\n"
     ]
    }
   ],
   "source": [
    "svc_clf_acc = svc_clf.score(X_test_ss, y_test_ss)\n",
    "\n",
    "print(f\"Mean Accuracy of SVC: {svc_clf_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def create_model(optimizer='adam', dropout_rate=0.0, neurons=1):\n",
    "    \n",
    "    input_size = len(X_train_array[0])\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(neurons, activation='relu', input_dim=input_size))\n",
    "    model.add(layers.Dropout(rate=dropout_rate))\n",
    "    model.add(layers.Dense(neurons, activation='relu', input_dim=input_size))\n",
    "    model.add(layers.Dropout(rate=dropout_rate))\n",
    "    model.add(layers.Dense(neurons, activation='relu', input_dim=input_size))\n",
    "    model.add(layers.Dropout(rate=dropout_rate))\n",
    "    model.add(layers.Dense(0.5*neurons, activation='relu', input_dim=input_size))\n",
    "    model.add(layers.Dropout(rate=dropout_rate))\n",
    "    model.add(layers.Dense(0.25*neurons, activation='relu', input_dim=input_size))\n",
    "    model.add(layers.Dropout(rate=dropout_rate))\n",
    "    model.add(layers.Dense(1, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model:\n",
      "acc: 0.524000013589859 using params: {'batch_size': 32, 'dropout_rate': 0.5, 'epochs': 10, 'neurons': 1000}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "param_grid = {\n",
    "    'dropout_rate': [0.5, 0.2],\n",
    "    'neurons': [1000, 250],\n",
    "    'batch_size': [32, 8],\n",
    "    'epochs': [10, 100]\n",
    "}\n",
    "\n",
    "# train on a sample of training data\n",
    "X_train_cv = X_train_array[0:250]\n",
    "y_train_cv = y_train_array[0:250]\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=1)\n",
    "grid_result = grid.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "print(\"Best Model:\")\n",
    "print(f\"acc: {grid_result.best_score_} using params: {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GridSearchCV, the Random Forest Classifier's accuracy increased from 0.9684 to 0.9812, while the Support Vector Classifier's accuracy decreased to 0.9368 from 0.9372. Due to time constraints, it is difficult to test neural network architectures, but it appears that it would require a much larger neural network in order to accurately classify the game winner based on the results of GridSearchCV. The neural network, despite having 4 additional layers, was only able to reach an accuracy of 0.524. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
